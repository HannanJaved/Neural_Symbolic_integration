{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment 1"
      ],
      "metadata": {
        "id": "SkN3ts8Y09lK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "gAuUpBcP2A76"
      },
      "execution_count": 160,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JmM9z88m2JuS",
        "outputId": "70cc2653-85a5-47ef-fdc3-ccb3efca1043"
      },
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to the dataset\n",
        "data_main_path  = '/content/drive/MyDrive/ColabNotebooks/Winter Semester 2023 24/NSI/Datasets-20231011/'"
      ],
      "metadata": {
        "id": "NyPd8vWy2Szo"
      },
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 1 - Spam"
      ],
      "metadata": {
        "id": "FzOwOlIy1FUp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to email dataset\n",
        "email_data_path = data_main_path + 'email/'"
      ],
      "metadata": {
        "id": "GN-2o8Rc5HHH"
      },
      "execution_count": 163,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 164,
      "metadata": {
        "id": "NI-72vqU05TT"
      },
      "outputs": [],
      "source": [
        "def parse_email_data(folder_path):\n",
        "    emails = []\n",
        "    labels = []  # Spam 1, Ham 0\n",
        "\n",
        "    # Iterate through ham emails\n",
        "    ham_folder_path = os.path.join(folder_path, 'ham')\n",
        "    for filename in os.listdir(ham_folder_path):\n",
        "        with open(os.path.join(ham_folder_path, filename), 'r', encoding='utf-8', errors='ignore') as file:\n",
        "            lines = file.readlines()\n",
        "            if len(lines) >= 3:\n",
        "                date = lines[0].strip()\n",
        "                subject = lines[1].strip()\n",
        "                body = ''.join(lines[2:]).strip()\n",
        "                emails.append({'date': date, 'subject': subject, 'body': body})\n",
        "                labels.append(0)\n",
        "\n",
        "    # Iterate through spam emails\n",
        "    spam_folder_path = os.path.join(folder_path, 'spam')\n",
        "    for filename in os.listdir(spam_folder_path):\n",
        "        with open(os.path.join(spam_folder_path, filename), 'r', encoding='utf-8', errors='ignore') as file:\n",
        "            lines = file.readlines()\n",
        "            if len(lines) >= 3:\n",
        "                date = lines[0].strip()\n",
        "                subject = lines[1].strip()\n",
        "                body = ''.join(lines[2:]).strip()\n",
        "                emails.append({'date': date, 'subject': subject, 'body': body})\n",
        "                labels.append(1)\n",
        "\n",
        "    return emails, labels"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_email_data = email_data_path + 'train/'\n",
        "parsed_emails_train, train_labels = parse_email_data(train_email_data)\n",
        "\n",
        "test_email_data = email_data_path + 'test/'\n",
        "parsed_emails_test, test_labels = parse_email_data(test_email_data)\n",
        "\n",
        "val_email_data = email_data_path + 'val/'\n",
        "parsed_emails_val, val_labels = parse_email_data(val_email_data)"
      ],
      "metadata": {
        "id": "vcZd1VTbQ4q0"
      },
      "execution_count": 186,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print some examples\n",
        "for parsed_emails in parsed_emails_train[:2]:\n",
        "  print(\"Date:\", parsed_emails['date'])\n",
        "  print(\"Subject:\", parsed_emails['subject'])\n",
        "  print(\"Body Text:\", parsed_emails['body'])\n",
        "  print()\n",
        "  print(\"Label:\", \"spam\" if train_labels[0] == 1 else \"ham\")\n",
        "  print(\"------\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uuG-KoujScFF",
        "outputId": "aba4d044-85b8-4ab8-e352-82eca6717dc6"
      },
      "execution_count": 187,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Date: Subject: pennzenergy property details\n",
            "Subject: - - - - - - - - - - - - - - - - - - - - - - forwarded by ami chokshi / corp / enron on 12 / 17 / 99 04 : 03\n",
            "Body Text: pm - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
            "dscottl @ . com on 12 / 14 / 99 10 : 56 : 01 am\n",
            "to : ami chokshi / corp / enron @ enron\n",
            "cc :\n",
            "subject : pennzenergy property details\n",
            "ami , attached is some more details on the devon south texas properties . let\n",
            "me\n",
            "know if you have any questions .\n",
            "david\n",
            "- devon stx . xls\n",
            "\n",
            "Label: ham\n",
            "------\n",
            "Date: Subject: hpl fuel gas buy - back for december 1999\n",
            "Subject: fyi :\n",
            "Body Text: - - - - - - - - - - - - - - - - - - - - - - forwarded by gregg lenart / hou / ect on 12 / 16 / 99 02 : 02 pm\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
            "enron north america corp .\n",
            "from : sally shuler @ enron 12 / 16 / 99 01 : 55 pm\n",
            "to : gregg lenart / hou / ect @ ect\n",
            "cc :\n",
            "subject : hpl fuel gas buy - back for december 1999\n",
            "- - - - - - - - - - - - - - - - - - - - - - forwarded by sally shuler / gpgfin / enron on 12 / 16 / 99\n",
            "02 : 02 pm - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
            "michael mitcham\n",
            "12 / 16 / 99 01 : 45 pm\n",
            "to : james prentice / gpgfin / enron @ enron , kerry roper / gpgfin / enron @ enron , sally\n",
            "shuler / gpgfin / enron @ enron , mark diedrich / gpgfin / enron @ enron , paul\n",
            "fox / ecf / enron @ enron\n",
            "cc :\n",
            "subject : hpl fuel gas buy - back for december 1999\n",
            "egp fuels fuels co . has sold back 7 , 000 mmbtu / day starting 12 / 17 through\n",
            "12 / 31 to hpl at $ 2 . 50 mmbtu .\n",
            "if anyone has any questions , please let me know .\n",
            "thanks\n",
            "\n",
            "Label: ham\n",
            "------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 2 - Shapes"
      ],
      "metadata": {
        "id": "wADi1Hz95CWw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to shapes dataset\n",
        "shapes_data_path = data_main_path + 'shapes/'"
      ],
      "metadata": {
        "id": "tHsrHb_65FoW"
      },
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to parse .wld files and extract features\n",
        "def parse_shapes_data(file_path):\n",
        "    with open(file_path, \"r\") as file:\n",
        "        data = json.load(file)\n",
        "        objects = []\n",
        "\n",
        "        # Iterate through each object in the data list\n",
        "        for item in data:\n",
        "            consts = item[\"Consts\"]\n",
        "            predicates = item[\"Predicates\"]\n",
        "            tags = item[\"Tags\"]\n",
        "\n",
        "            # Extract features for each object\n",
        "            const_label = consts[0]\n",
        "            shape, size = predicates\n",
        "            position = tuple(tags)\n",
        "\n",
        "            objects.append({\n",
        "                'label': const_label,\n",
        "                'shape': shape,\n",
        "                'size': size,\n",
        "                'position': position\n",
        "            })\n",
        "\n",
        "        return objects"
      ],
      "metadata": {
        "id": "j9A2EbkKFXWH"
      },
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to parse .wld files in a folder\n",
        "def parse_shapes_files_in_folder(folder_path):\n",
        "    parsed_objects = []\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.endswith(\".wld\"):\n",
        "            file_path = os.path.join(folder_path, filename)\n",
        "            objects_data = parse_shapes_data(file_path)\n",
        "            parsed_objects.extend(objects_data)\n",
        "    return parsed_objects"
      ],
      "metadata": {
        "id": "rv7U_QX8GGeI"
      },
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parsed_objects_train = parse_shapes_files_in_folder(os.path.join(shapes_data_path, 'train'))\n",
        "parsed_objects_test = parse_shapes_files_in_folder(os.path.join(shapes_data_path, 'test'))\n",
        "parsed_objects_val = parse_shapes_files_in_folder(os.path.join(shapes_data_path, 'val'))"
      ],
      "metadata": {
        "id": "4qgPYfFcGFDa"
      },
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print some examples\n",
        "for parsed_objects in parsed_objects_train[:5]:\n",
        "  print(\"Shape:\", parsed_objects['shape'])\n",
        "  print(\"Label:\", parsed_objects['label'])\n",
        "  print(\"Size:\", parsed_objects['size'])\n",
        "  print(\"Position:\", parsed_objects['position'])\n",
        "  print(\"------\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hlD0uEO-IibP",
        "outputId": "b38dde71-bb91-4265-91ac-1b6ece78b8e2"
      },
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape: Tet\n",
            "Label: e\n",
            "Size: Large\n",
            "Position: (0, 0)\n",
            "------\n",
            "Shape: Tet\n",
            "Label: b\n",
            "Size: Large\n",
            "Position: (0, 2)\n",
            "------\n",
            "Shape: Dodec\n",
            "Label: d\n",
            "Size: Medium\n",
            "Position: (1, 1)\n",
            "------\n",
            "Shape: Dodec\n",
            "Label: c\n",
            "Size: Medium\n",
            "Position: (2, 1)\n",
            "------\n",
            "Shape: Dodec\n",
            "Label: a\n",
            "Size: Medium\n",
            "Position: (2, 3)\n",
            "------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 3 - Math"
      ],
      "metadata": {
        "id": "qKkSSXjX6Boh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to math dataset\n",
        "math_data_path = data_main_path + 'math/'"
      ],
      "metadata": {
        "id": "mDLdB2UU6LCG"
      },
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to parse the expressions and compute the results\n",
        "def parse_and_compute_expressions(file_path):\n",
        "    with open(file_path, \"r\") as file:\n",
        "        lines = file.readlines()\n",
        "        expressions = []\n",
        "\n",
        "        for line in lines:\n",
        "            # Extract arithmetic expression from the line\n",
        "            expression = line.strip()\n",
        "\n",
        "            # Parse the expression into a tree using AST\n",
        "            parse_tree = ast.parse(expression, mode='eval')\n",
        "\n",
        "            # Compute the result of the expression\n",
        "            result = eval(compile(parse_tree, filename=\"\", mode='eval'))\n",
        "            expressions.append({\n",
        "                'expression': expression,\n",
        "                'result': result\n",
        "            })\n",
        "\n",
        "        return expressions"
      ],
      "metadata": {
        "id": "yRNRvVFK6Df5"
      },
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_and_solve_expression(file_path):\n",
        "    parsed_expressions = parse_and_compute_expressions(file_path)\n",
        "    return parsed_expressions\n",
        "\n",
        "# Train, test, and val\n",
        "parsed_expressions_train = parse_and_solve_expression(os.path.join(math_data_path, 'train.txt'))\n",
        "parsed_expressions_test = parse_and_solve_expression(os.path.join(math_data_path, 'test.txt'))\n",
        "parsed_expressions_val = parse_and_solve_expression(os.path.join(math_data_path, 'val.txt'))"
      ],
      "metadata": {
        "id": "dbzOQeyH7hNr"
      },
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print some examples\n",
        "for parsed_expressions in parsed_expressions_train[:5]:\n",
        "  print(\"Expression:\", parsed_expressions['expression'])\n",
        "  print(\"Result:\", parsed_expressions['result'])\n",
        "  print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AAEzFXZS6lgm",
        "outputId": "f8194421-e0d2-4bad-e6e9-3d25532aa522"
      },
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Expression: (-2 * 9 * 5)\n",
            "Result: -90\n",
            "\n",
            "Expression: (4 * 8 * 5)\n",
            "Result: 160\n",
            "\n",
            "Expression: (-6 * ((-1 - 1) - -6))\n",
            "Result: -24\n",
            "\n",
            "Expression: (-9 * 4 * (1 - (1 * -8)) * 0)\n",
            "Result: 0\n",
            "\n",
            "Expression: (8 * -4 * 5)\n",
            "Result: -160\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 4 - Iris"
      ],
      "metadata": {
        "id": "voRIHxcR8Crj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to iris dataset\n",
        "iris_data_path = data_main_path + 'iris/'"
      ],
      "metadata": {
        "id": "yP8UsK0I8GiQ"
      },
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task a - Parse the data"
      ],
      "metadata": {
        "id": "McTC4kyr-Xsq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_iris_data(file_path, has_column_names=False):\n",
        "    if has_column_names:\n",
        "      # Since Test and Val files dont have column names\n",
        "        data = pd.read_csv(file_path, index_col=0)\n",
        "    else:\n",
        "        column_names = ['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm', 'Species']\n",
        "        data = pd.read_csv(file_path, names=column_names, index_col=0)\n",
        "    features = data[['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']]\n",
        "    labels = data['Species']\n",
        "    return features, labels\n",
        "\n",
        "# train, test, and val\n",
        "train_features, train_labels = parse_iris_data(os.path.join(iris_data_path, 'train.csv'), has_column_names=True)\n",
        "test_features, test_labels = parse_iris_data(os.path.join(iris_data_path, 'test.csv'))\n",
        "val_features, val_labels = parse_iris_data(os.path.join(iris_data_path, 'val.csv'))"
      ],
      "metadata": {
        "id": "vJsH43pW95Qt"
      },
      "execution_count": 183,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print some examples\n",
        "print(\"Train Features:\")\n",
        "print(train_features.head())\n",
        "\n",
        "print()\n",
        "\n",
        "print(\"Train Labels:\")\n",
        "print(train_labels.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y81Mh2k39hxr",
        "outputId": "55fe2f93-da7c-4924-e0f4-e277cc7f49a9"
      },
      "execution_count": 184,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Features:\n",
            "    SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm\n",
            "Id                                                          \n",
            "1             5.1           3.5            1.4           0.2\n",
            "3             4.7           3.2            1.3           0.2\n",
            "4             4.6           3.1            1.5           0.2\n",
            "5             5.0           3.6            1.4           0.2\n",
            "6             5.4           3.9            1.7           0.4\n",
            "\n",
            "Train Labels:\n",
            "Id\n",
            "1    Iris-setosa\n",
            "3    Iris-setosa\n",
            "4    Iris-setosa\n",
            "5    Iris-setosa\n",
            "6    Iris-setosa\n",
            "Name: Species, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task b - Feed-forward Network"
      ],
      "metadata": {
        "id": "DfXOwGqZ-asS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert features and labels to Tensors\n",
        "train_features_tensor = torch.tensor(train_features.values, dtype=torch.float32)\n",
        "train_labels_tensor = torch.tensor(pd.Categorical(train_labels).codes, dtype=torch.long)\n",
        "\n",
        "test_features_tensor = torch.tensor(test_features.values, dtype=torch.float32)\n",
        "test_labels_tensor = torch.tensor(pd.Categorical(test_labels).codes, dtype=torch.long)\n",
        "\n",
        "val_features_tensor = torch.tensor(val_features.values, dtype=torch.float32)\n",
        "val_labels_tensor = torch.tensor(pd.Categorical(val_labels).codes, dtype=torch.long)"
      ],
      "metadata": {
        "id": "FCi8tEWsAoD9"
      },
      "execution_count": 169,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparams\n",
        "\n",
        "input_size = 4\n",
        "output_size = 3\n",
        "hidden_size = 512\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "cZD3wP93Axxx"
      },
      "execution_count": 170,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = nn.Sequential(\n",
        "    nn.Linear(input_size, hidden_size),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(hidden_size, output_size),\n",
        "    nn.Softmax(dim=1)\n",
        ")"
      ],
      "metadata": {
        "id": "dveWZjh1-qha"
      },
      "execution_count": 171,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "mqAGggGgA9Zu"
      },
      "execution_count": 172,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training\n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    outputs = model(train_features_tensor)\n",
        "    loss = criterion(outputs, train_labels_tensor)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Print the loss every 10 epochs\n",
        "    if (epoch+1) % 10 == 0:\n",
        "        print(f'At Epoch {epoch+1} the Loss is: {loss.item():.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5lnHO-8EAvAH",
        "outputId": "fca74b20-546e-414d-c65c-107cf83616ab"
      },
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "At Epoch 10 the Loss is: 0.9362\n",
            "At Epoch 20 the Loss is: 0.8179\n",
            "At Epoch 30 the Loss is: 0.7582\n",
            "At Epoch 40 the Loss is: 0.7164\n",
            "At Epoch 50 the Loss is: 0.6856\n",
            "At Epoch 60 the Loss is: 0.6642\n",
            "At Epoch 70 the Loss is: 0.6484\n",
            "At Epoch 80 the Loss is: 0.6365\n",
            "At Epoch 90 the Loss is: 0.6273\n",
            "At Epoch 100 the Loss is: 0.6201\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test Set Performance\n",
        "\n",
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    predictions = model(test_features_tensor)\n",
        "    _, predicted_classes = torch.max(predictions, 1)\n",
        "    accuracy = torch.sum(predicted_classes == test_labels_tensor).item() / test_labels_tensor.size(0)\n",
        "    print(f'Test Accuracy: {accuracy*100:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MnkcYkNUAmDB",
        "outputId": "73ed1dfb-4694-4d60-96a0-bddc0e13d318"
      },
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 94.74%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Validation Set Performance\n",
        "\n",
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    predictions = model(val_features_tensor)\n",
        "    _, predicted_classes = torch.max(predictions, 1)\n",
        "    accuracy = torch.sum(predicted_classes == val_labels_tensor).item() / val_labels_tensor.size(0)\n",
        "    print(f'Val Accuracy: {accuracy*100:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RJNLYbl1_uRn",
        "outputId": "5890b401-7b77-43c0-f602-7551dccc3b93"
      },
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Accuracy: 90.91%\n"
          ]
        }
      ]
    }
  ]
}