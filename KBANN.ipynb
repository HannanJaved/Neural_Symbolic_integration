{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from sklearn import mixture\n",
        "import torch as pt\n",
        "from torch import nn\n",
        "import json\n",
        "\n",
        "DEFAULT_WEIGHT = 4.0"
      ],
      "metadata": {
        "id": "7bi5BRcAr6dg"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WGumUGMLsxRD",
        "outputId": "ab24a892-1c75-425b-dfec-848ae243686d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dir_path = '/content/drive/MyDrive/ColabNotebooks/Winter Semester 2023 24/NSI/KBANN'"
      ],
      "metadata": {
        "id": "2h-oE5oks284"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cluster_weights(links, threshold):\n",
        "    \"\"\"Cluster weights (links) using Gaussian mixture model with EM.\n",
        "\n",
        "    It also determines the number of clusters using Bayesian information\n",
        "    criterion and sets the weight of all links in each cluster to the\n",
        "    average of each cluster's weight.\n",
        "\n",
        "    Args:\n",
        "        links: weights of links connected to a single unit.\n",
        "        threshold: bias value.\n",
        "    Returns:\n",
        "        Clustered weights\n",
        "    \"\"\"\n",
        "\n",
        "    weights = np.transpose(np.array([links]))\n",
        "\n",
        "    # Clustering links\n",
        "    n = len(links)\n",
        "    MIN_NUM_SAMPLES = 2\n",
        "    if n > MIN_NUM_SAMPLES:\n",
        "        # Fit a mixture of Gaussians with EM\n",
        "        lowest_bic = np.infty\n",
        "        bic = []\n",
        "        for n_components in range(2, n):\n",
        "            gmm = mixture.GaussianMixture(\n",
        "                n_components=n_components, covariance_type=\"full\"\n",
        "            )\n",
        "            gmm.fit(weights)\n",
        "            # Bayesian information criterion\n",
        "            bic.append(gmm.bic(weights))\n",
        "            if bic[-1] < lowest_bic:\n",
        "                lowest_bic = bic[-1]\n",
        "                best_gmm = gmm\n",
        "\n",
        "        # Average weights\n",
        "        ids = best_gmm.predict(weights)\n",
        "        unique_ids = list(set(ids))\n",
        "\n",
        "        for i in unique_ids:\n",
        "            indices = ids == i\n",
        "            average_weight = np.sum(links[indices]) / len(links[indices])\n",
        "            links[indices] = average_weight\n",
        "\n",
        "        return links, ids\n",
        "    elif n == 2:\n",
        "        return links, np.array([0, 1])\n",
        "    else:\n",
        "        return links, np.zeros(len(links))"
      ],
      "metadata": {
        "id": "JWB_ADM6r9nl"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data_shapes(data_folder):\n",
        "    \"\"\"Preprocess the shapes dataset for KBANN\n",
        "\n",
        "    Args:\n",
        "        data_folder: Path to the folder containing train, test, and val subfolders\n",
        "    Returns:\n",
        "        X: List of feature vectors (Predicates and Tags) for each object\n",
        "        y: List of labels (Cube or not)\n",
        "        feature_names: List of feature names\n",
        "    \"\"\"\n",
        "    features = []\n",
        "    X = []\n",
        "    y = []\n",
        "\n",
        "    # for subset in ['train', 'test', 'val']:\n",
        "    for subset in ['train']:\n",
        "        subset_path = os.path.join(data_folder, subset)\n",
        "\n",
        "        for filename in os.listdir(subset_path):\n",
        "            if filename.endswith(\".wld\"):\n",
        "                filepath = os.path.join(subset_path, filename)\n",
        "\n",
        "                with open(filepath, \"r\") as file:\n",
        "                    world_data = json.load(file)\n",
        "\n",
        "                    for obj_data in world_data:\n",
        "                        label = obj_data['Consts']\n",
        "                        shape = obj_data['Predicates'][0]\n",
        "                        size = obj_data['Predicates'][1]\n",
        "                        x, y_coord = obj_data['Tags']\n",
        "\n",
        "                        # Assuming we're predicting whether the object labeled \"c\" is a cube\n",
        "                        is_cube = 1 if label == 'c' and shape == 'Cube' else 0\n",
        "\n",
        "                        # Feature vector: [shape, size, x, y]\n",
        "                        feature_vector = [shape, size, x, y_coord]\n",
        "\n",
        "                        if not features:\n",
        "                            features = [\"Shape\", \"Size\", \"X\", \"Y\"]\n",
        "\n",
        "                        X.append(feature_vector)\n",
        "                        y.append(is_cube)\n",
        "\n",
        "    return np.array(X), np.array(y), features"
      ],
      "metadata": {
        "id": "LirsWkkcjnt0"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(filename):\n",
        "    \"\"\"Read features and training samples from dataset\n",
        "\n",
        "    Args:\n",
        "        filename: file name to load data\n",
        "    Returns:\n",
        "        y: labels\n",
        "        X: Training data\n",
        "        feature_names: a list of feature names\n",
        "    \"\"\"\n",
        "    file = open(filename, \"rt\", encoding=\"UTF8\")\n",
        "\n",
        "    features = []\n",
        "    X = []\n",
        "    y = []\n",
        "    for line in file:\n",
        "        line = line.replace(\"\\n\", \"\")\n",
        "        row = [s.strip() for s in line.split(\",\")]\n",
        "        if not features:\n",
        "            # The first line is a list of feature names\n",
        "            features = row\n",
        "        else:\n",
        "            # The rest of the lines is training data\n",
        "            X.append([float(s) for s in row[:-1]])\n",
        "            # The last column stores labels\n",
        "            y.append(row[-1])\n",
        "    file.close()\n",
        "\n",
        "    return np.array(X), np.transpose(np.array([y])), features"
      ],
      "metadata": {
        "id": "l74JaPuGsD-y"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Literal:\n",
        "    \"\"\"Literal object\n",
        "\n",
        "    Attributes:\n",
        "        name: the name of predicate\n",
        "        negated: indicates whether the predicate is negated.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, name, negated=False):\n",
        "        self.name = name\n",
        "        self.negated = negated"
      ],
      "metadata": {
        "id": "wtoVCzrSsJrB"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Rule:\n",
        "    \"\"\"First order rule\n",
        "\n",
        "    Attributes:\n",
        "        head: the consequent of the rule\n",
        "        body: the antecedents of the rule\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, head, body):\n",
        "        self.head = head\n",
        "        self.body = body"
      ],
      "metadata": {
        "id": "jW_NSIAxsK6n"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_rules(filename):\n",
        "    \"\"\"Load rules from a file\n",
        "\n",
        "    Create a set of rule objects with head and body\n",
        "    elements from rule file\n",
        "\n",
        "    Args:\n",
        "        filename:\n",
        "\n",
        "    Returns:\n",
        "        A list of rules\n",
        "    \"\"\"\n",
        "\n",
        "    def cleanse(str):\n",
        "        \"\"\"Sanitize a string rule and remove stopwords\"\"\"\n",
        "        rep = [\"\\n\", \"-\", \" \", \".\"]\n",
        "        for r in rep:\n",
        "            str = str.replace(r, \"\")\n",
        "        return str\n",
        "\n",
        "    file = open(filename, \"rt\", encoding=\"UTF8\")\n",
        "    ruleset = []\n",
        "    for line in file:\n",
        "        tokens = line.split(\":\")\n",
        "        head = Literal(cleanse(tokens[0]))\n",
        "        body = []\n",
        "        for obj in tokens[1].split(\",\"):\n",
        "            obj = cleanse(obj)\n",
        "            negated = False\n",
        "            if obj.startswith(\"not\"):\n",
        "                negated = True\n",
        "                obj = obj.replace(\"not\", \"\")\n",
        "            predicate = Literal(cleanse(obj), negated=negated)\n",
        "            body.append(predicate)\n",
        "        rule = Rule(head, body)\n",
        "        ruleset.append(rule)\n",
        "    file.close()\n",
        "    return ruleset"
      ],
      "metadata": {
        "id": "Qn995AG-sMlA"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rewrite_rules(ruleset):\n",
        "    \"\"\"Scan every rule and rewrite the ones with the same consequents\n",
        "\n",
        "    It implements Towell's rewritting algorithm. If there is more than one\n",
        "    rule to consequent, then rewrite it as two rules.\n",
        "\n",
        "    Args:\n",
        "        ruleset: a set of rules\n",
        "    Returns:\n",
        "        A set of rewritten rules. For example:\n",
        "\n",
        "        A :- B, C.\n",
        "        A :- D, E.\n",
        "        are written as\n",
        "        A :- A'.\n",
        "        A' :- B, C.\n",
        "        A :- A''.\n",
        "        A'' :- D, E.\n",
        "    \"\"\"\n",
        "\n",
        "    # Dict is a dictionary that stores consequences along with their occurrence\n",
        "    # Keys are to the consequences (head) and values are the occurrence\n",
        "    dict = {}\n",
        "    for rule in ruleset:\n",
        "        if rule.head.name not in dict:\n",
        "            dict[rule.head.name] = 1\n",
        "        else:\n",
        "            dict[rule.head.name] += 1\n",
        "\n",
        "    # Rewrite rules that conclude the same consequences\n",
        "    rewritten_rules = []\n",
        "    i = len(ruleset)\n",
        "    for rule in ruleset[:]:\n",
        "        if dict[rule.head.name] > 1:\n",
        "            # Create a new intermediate consequent\n",
        "            new_predicate = Literal(rule.head.name + str(i))\n",
        "            # Create two new rules for the consequence and antecedents\n",
        "            rewritten_rules.append(Rule(rule.head, [new_predicate]))\n",
        "            rewritten_rules.append(Rule(new_predicate, rule.body))\n",
        "            ruleset.remove(rule)\n",
        "            i += 1\n",
        "    del dict\n",
        "\n",
        "    return ruleset + rewritten_rules"
      ],
      "metadata": {
        "id": "vjrijgYasOL3"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_antecedents(rules):\n",
        "    \"\"\"Retrieve all the antecedents from a set of rules\n",
        "\n",
        "    Args:\n",
        "        rules: a set of rules.\n",
        "\n",
        "    Returns:\n",
        "        all_antecedents: Retrieves and flattens rules' antecedents.\n",
        "        Only, antecedent names are returned.\n",
        "    \"\"\"\n",
        "\n",
        "    all_antecedents = []\n",
        "    for rule in rules:\n",
        "        for predicate in rule.body:\n",
        "            if predicate.name not in all_antecedents:\n",
        "                all_antecedents.append(predicate.name)\n",
        "    return all_antecedents"
      ],
      "metadata": {
        "id": "CtQsu_INsP6G"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_consequents(rules):\n",
        "    \"\"\"Retrieve all the consequents from a set of rules\n",
        "\n",
        "    Args:\n",
        "        rules: a set of rules.\n",
        "\n",
        "    Returns:\n",
        "        all_consequents: Retrieves and flattens rules' consequents.\n",
        "        Only, consequent names are returned.\n",
        "    \"\"\"\n",
        "\n",
        "    all_consequents = []\n",
        "    for rule in rules:\n",
        "        if rule.head.name not in all_consequents:\n",
        "            all_consequents.append(rule.head.name)\n",
        "    return all_consequents"
      ],
      "metadata": {
        "id": "osY7VhvlsRfE"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rule_to_network(ruleset):\n",
        "    \"\"\"Translating rules to network (Towell's mapping algorithm)\n",
        "\n",
        "    Establishes a mapping between a set of rules and a neural network.\n",
        "    This mapping creates layers, weights and biases for the neural network.\n",
        "\n",
        "    Args:\n",
        "        ruleset: a set of rewritten rules.\n",
        "\n",
        "    Returns:\n",
        "        weights: network weights\n",
        "        biases: network biases\n",
        "\n",
        "        Weights and biases are initialized corresponding to disjunctive and\n",
        "        conjunctive rules\n",
        "    \"\"\"\n",
        "\n",
        "    # Create network layers from rules\n",
        "    rule_layers = []\n",
        "    l = 0\n",
        "    copied_rules = ruleset.copy()\n",
        "    while len(copied_rules) > 0:\n",
        "        if l == 0:\n",
        "            all_antecedents = get_antecedents(copied_rules)\n",
        "        else:\n",
        "            all_antecedents = get_antecedents(rule_layers[-1])\n",
        "\n",
        "        rule_layer = []\n",
        "        for rule in copied_rules[:]:\n",
        "            if rule.head.name not in all_antecedents:\n",
        "                rule_layer.append(rule)\n",
        "                copied_rules.remove(rule)\n",
        "        del all_antecedents[:]\n",
        "        rule_layers.append(rule_layer)\n",
        "\n",
        "    # Reverse the order of the list\n",
        "    rule_layers = rule_layers[::-1]\n",
        "\n",
        "    # Create weights and biases for each layer in the network\n",
        "    omega = DEFAULT_WEIGHT\n",
        "    weights = []\n",
        "    biases = []\n",
        "    layers = []\n",
        "    last_layer = []\n",
        "\n",
        "    for rule_layer in rule_layers:\n",
        "\n",
        "        current_layer = get_antecedents(rule_layer)\n",
        "        next_layer = get_consequents(rule_layer)\n",
        "\n",
        "        for unit in current_layer:\n",
        "            if unit not in last_layer:\n",
        "                last_layer.append(unit)\n",
        "        current_layer = last_layer.copy()\n",
        "\n",
        "        layers.extend([current_layer, next_layer])\n",
        "        last_layer = next_layer.copy()\n",
        "\n",
        "        # Store the occurrence of consequences. For example,\n",
        "        # if a consequent occurred more than one, then it is a disjunctive rule\n",
        "        dict = {}\n",
        "        for rule in rule_layer:\n",
        "            if rule.head.name not in dict:\n",
        "                dict[rule.head.name] = 1\n",
        "            else:\n",
        "                dict[rule.head.name] += 1\n",
        "\n",
        "        weight = np.zeros([len(current_layer), len(next_layer)])\n",
        "        bias = np.zeros(len(next_layer))\n",
        "\n",
        "        for rule in rule_layer:\n",
        "\n",
        "            j = next_layer.index(rule.head.name)\n",
        "            for predicate in rule.body:\n",
        "                i = current_layer.index(predicate.name)\n",
        "                if predicate.negated:\n",
        "                    weight[i][j] = -omega\n",
        "                else:\n",
        "                    weight[i][j] = omega\n",
        "\n",
        "            if dict[rule.head.name] > 1:\n",
        "                bias[j] = 0.5 * omega\n",
        "            else:\n",
        "                p = len(rule.body)\n",
        "                bias[j] = (p - 0.5) * omega\n",
        "\n",
        "        weights.append(np.array(weight))\n",
        "        biases.append(np.array([bias]))\n",
        "\n",
        "    return weights, biases, layers"
      ],
      "metadata": {
        "id": "AyC5Bs4TsUB1"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data(dataset, feature_names, layers):\n",
        "    \"\"\"Preprocessing input data\"\"\"\n",
        "\n",
        "    last_layer = []\n",
        "    X = []\n",
        "    i = 1\n",
        "    for layer in layers:\n",
        "        indices = []\n",
        "        if i == 1:\n",
        "            # input layer\n",
        "            indices = [feature_names.index(unit) for unit in layer]\n",
        "            X.append(dataset[:, indices])\n",
        "\n",
        "        elif i % 2 != 0 and len(last_layer) > 0:\n",
        "            # hidden and output layer\n",
        "            hidden_input = [unit for unit in layer if unit not in last_layer]\n",
        "            indices = [feature_names.index(unit) for unit in hidden_input]\n",
        "            x = dataset[:, indices]\n",
        "            n = len(x)\n",
        "            m = len(x[0])\n",
        "            X.append(x + 0.00001 * np.random.rand(n, m))\n",
        "        else:\n",
        "            last_layer = layer\n",
        "        i += 1\n",
        "\n",
        "    return X"
      ],
      "metadata": {
        "id": "2nU2ydWSsVuF"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def eliminate_weights(weights, biases):\n",
        "    \"\"\"Eliminate weights that are not contributing to the output\"\"\"\n",
        "    cluster_ids = []\n",
        "    for i in range(len(weights)):\n",
        "        cluster = []\n",
        "        for j in range(weights[i].shape[1]):\n",
        "            b = biases[i][0, j]\n",
        "            (_w, ids) = cluster_weights(weights[i][:, j], b)\n",
        "            weights[i][:, j] = list(_w)\n",
        "            cluster.append(ids)\n",
        "        cluster_ids.append(cluster)\n",
        "\n",
        "    return weights, biases, cluster_ids"
      ],
      "metadata": {
        "id": "5OuzEmyysXdh"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def network_to_rule(weights, biases, cluster_indices, layers):\n",
        "    \"\"\"Translate network to rule.\n",
        "\n",
        "    Extract rules from neural network, specifically weights.\n",
        "\n",
        "    Args:\n",
        "        weights: a set of weights\n",
        "        biases: a set of biases\n",
        "        cluster_indices: a set of indices that clusters weights\n",
        "        layers: layers of neural network units\n",
        "\n",
        "    Returns:\n",
        "        a set of rules extracted from the neural network\n",
        "    \"\"\"\n",
        "\n",
        "    rules = []\n",
        "    layers = np.array(layers)\n",
        "    weight_range = range(0, len(weights))\n",
        "    layer_range = range(0, len(layers), 2)\n",
        "    for i, l in zip(weight_range, layer_range):\n",
        "        current_layer = np.array(layers[l])\n",
        "        next_layer = layers[l + 1]\n",
        "        for j in range(weights[i].shape[1]):\n",
        "            b = biases[i][0, j]\n",
        "            w = weights[i][:, j]\n",
        "            head = next_layer[j]\n",
        "            indices = cluster_indices[i][j]\n",
        "            unique_ids = list(set(indices))\n",
        "            body = \"\"\n",
        "            for id in unique_ids:\n",
        "                if body != \"\":\n",
        "                    body += \" + \"\n",
        "                matched_indices = indices == id\n",
        "                antecedents = current_layer[matched_indices]\n",
        "                threshold = w[matched_indices]\n",
        "                body += str(threshold[0]) + \" * nt(\" + \",\".join(antecedents) + \")\"\n",
        "            new_rule = head + \" :- \" + str(b) + \" < \" + body\n",
        "            rules.append(new_rule)\n",
        "\n",
        "            print(head + \" = 0\")\n",
        "            print(\"if \" + str(b) + \" < \" + body + \":\")\n",
        "            print(\"\\t\" + head + \" = 1\")\n",
        "    return rules"
      ],
      "metadata": {
        "id": "7u4QUda9sZfk"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_input_units(weights, layers, feature_names):\n",
        "    \"\"\"Add input features not referred by the rule set\n",
        "\n",
        "    This addition is necessary because a set of rules that\n",
        "    is only approximately correct may not identify every input\n",
        "    that is required for correctly learning a concept.\n",
        "    \"\"\"\n",
        "\n",
        "    additional_units = feature_names.copy()\n",
        "\n",
        "    for layer in layers:\n",
        "        for unit in layer:\n",
        "            if unit in feature_names:\n",
        "                additional_units.remove(unit)\n",
        "\n",
        "    w = weights[0]\n",
        "    zeros = np.zeros((len(additional_units), w.shape[1]))\n",
        "    weights[0] = np.row_stack([w, zeros])\n",
        "    layers[0] += additional_units\n",
        "\n",
        "    return weights, layers"
      ],
      "metadata": {
        "id": "GeCkYpjDsbFT"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_hidden_units(weights, biases, layers):\n",
        "    \"\"\"Add units to hidden layers\"\"\"\n",
        "    w1 = weights[0]\n",
        "    w2 = weights[1]\n",
        "    zeros1 = np.zeros((w1.shape[0], 3))\n",
        "    weights[0] = np.column_stack([w1, zeros1])\n",
        "    zeros2 = np.zeros((3, 1))\n",
        "\n",
        "    weights[1] = np.row_stack([w2, zeros2])\n",
        "    b = biases[0]\n",
        "    biases[0] = np.column_stack([b, np.zeros((1, 3))])\n",
        "\n",
        "    layers[1].insert(len(layers[1]), \"head1\")\n",
        "    layers[1].insert(len(layers[1]), \"head2\")\n",
        "    layers[1].insert(len(layers[1]), \"head3\")\n",
        "\n",
        "    layers[2].insert(len(layers[2]), \"head1\")\n",
        "    layers[2].insert(len(layers[2]), \"head2\")\n",
        "    layers[2].insert(len(layers[2]), \"head3\")\n",
        "\n",
        "    return weights, biases, layers"
      ],
      "metadata": {
        "id": "uA_Adm-Mscnr"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def simplify_rules(rules):\n",
        "    simplified_rules = []\n",
        "\n",
        "    for rule in rules:\n",
        "        head, conditions = rule.split(\":-\")\n",
        "        head = head.strip()\n",
        "        conditions = conditions.strip().split(\"+\")\n",
        "        weight, *antecedents = [condition.strip() for condition in conditions]\n",
        "\n",
        "        # Extracting weight and comparison sign\n",
        "        weight, comparison_sign = (\n",
        "            weight.split(\"<\") if \"<\" in weight else [weight, '<']\n",
        "        ) if \"<\" in weight or \">\" in weight else [weight, '<']\n",
        "\n",
        "        weight = float(weight.strip())\n",
        "        comparison_sign = comparison_sign.strip()\n",
        "\n",
        "        simplified_rule = f\"{head} Rule:\\n\"\n",
        "        simplified_rule += f\"A person {head.lower()} if the weighted sum of the following conditions is {comparison_sign} than {weight:.3f}.\\n\"\n",
        "        simplified_rule += \"Conditions:\\n\"\n",
        "\n",
        "        for antecedent in antecedents:\n",
        "            terms = antecedent.strip().split(\"*\")\n",
        "            term_weight = float(terms[0].strip())\n",
        "            predicates = [predicate.strip() for predicate in terms[1].split(\",\")]\n",
        "\n",
        "            simplified_condition = f\"{'' if comparison_sign == '<' else 'Not '}\"\n",
        "            simplified_condition += \"(\" + \" and \".join(predicates) + \")\" if comparison_sign == '<' else \"(\".join([pred for pred in predicates]) + \")\"\n",
        "            simplified_condition += f\" weighted by {term_weight:.3f}\"\n",
        "\n",
        "            simplified_rule += simplified_condition + \"\\n\"\n",
        "\n",
        "        simplified_rules.append(simplified_rule)\n",
        "\n",
        "    return simplified_rules"
      ],
      "metadata": {
        "id": "iKECFsV60OaK"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save(rules, filepath):\n",
        "    with open(filepath, \"w\") as f:\n",
        "        for row in rules:\n",
        "            f.write(repr(str(row)) + \"\\n\")"
      ],
      "metadata": {
        "id": "ZxfE_-xTshrC"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class KBANN(nn.Module):\n",
        "    \"\"\"Knowledge base artificial neural network\n",
        "\n",
        "    Create KBANN network on the tensorflow framework.\n",
        "\n",
        "    Attributes:\n",
        "        weights: a set of tensors containing all weights in the network\n",
        "        biases: a set of tensors containing all biases\n",
        "        num_layers: the number of layers in the network\n",
        "        input_data: a set of tensors containing input data in each layer.\n",
        "            If there is no input for the layer, it stores an empty list\n",
        "        input_mask: a list of booleans that indicates in which layer it feeds the network\n",
        "        learning_rate: learning rate for optimization\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, weights, biases, fix_weights=False):\n",
        "        \"\"\"Set network parameters\"\"\"\n",
        "\n",
        "        super().__init__()\n",
        "        self.w = nn.ParameterList(\n",
        "            [\n",
        "                nn.Parameter(\n",
        "                    w + 0.1 * pt.rand((len(w), len(w[0]))),\n",
        "                    requires_grad=not fix_weights,\n",
        "                )\n",
        "                for w in weights\n",
        "            ]\n",
        "        )\n",
        "        self.b = nn.ParameterList(\n",
        "            [\n",
        "                nn.Parameter(b + 0.1 * pt.rand((1, len(b))), requires_grad=True)\n",
        "                for b in biases\n",
        "            ]\n",
        "        )\n",
        "        self.num_layers = len(weights)\n",
        "        self.dropout = nn.Dropout(p=0.1)\n",
        "\n",
        "    def forward(self, input_data, input_mask=None, dropout=True):\n",
        "        \"\"\"Implements the forward propagation\"\"\"\n",
        "\n",
        "        activations = [pt.sigmoid(pt.matmul(input_data, self.w[0]) - self.b[0])]\n",
        "        for i in range(1, self.num_layers):\n",
        "            if input_mask and input_mask[i]:\n",
        "                input_tensor = pt.concat([activations[-1], input_data[i]], dim=1)\n",
        "            else:\n",
        "                input_tensor = activations[-1]\n",
        "            if dropout:\n",
        "                input_tensor = self.dropout(input_tensor)\n",
        "            activation = pt.sigmoid(pt.matmul(input_tensor, self.w[i]) - self.b[i])\n",
        "            activations.append(activation)\n",
        "        return activations[-1]\n",
        "\n",
        "    @property\n",
        "    def weights(self):\n",
        "        return [w.detach().numpy() for w in self.w]\n",
        "\n",
        "    @property\n",
        "    def biases(self):\n",
        "        return [b.detach().numpy() for b in self.b]"
      ],
      "metadata": {
        "id": "MCdBoKyysj6t"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def display(arrays):\n",
        "    for array in arrays:\n",
        "        print(array)"
      ],
      "metadata": {
        "id": "Tc8Ut9Wgsl1M"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, X, y, training_epochs, optimizer, criterion):\n",
        "\n",
        "    # Refine rules\n",
        "    for epoch in range(training_epochs):\n",
        "        optimizer.zero_grad()\n",
        "        pred = model(X)\n",
        "        l = criterion(pred, y)\n",
        "        l.backward()\n",
        "        optimizer.step()\n",
        "        print(\"Epoch %d: Loss = %.9f\" % (epoch, l))"
      ],
      "metadata": {
        "id": "Q0OvIS1jsnNr"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main(\n",
        "    X,\n",
        "    y,\n",
        "    feature_names,\n",
        "    training_epochs,\n",
        "    rule_file_path,\n",
        "    atoms_to_add,\n",
        "):\n",
        "    # Translate rules to a network\n",
        "\n",
        "    ruleset = load_rules(rule_file_path)\n",
        "    ruleset = rewrite_rules(ruleset)\n",
        "    weights, biases, layers = rule_to_network(ruleset)\n",
        "\n",
        "    display(layers)\n",
        "    print(\"---------------------\")\n",
        "    # Add input features not referred by the rule set\n",
        "    # weights, layers = add_input_units(weights, layers, ['complete_course'])\n",
        "    weights, layers = add_input_units(weights, layers, atoms_to_add)\n",
        "\n",
        "    # Add hidden units not specified by the initial rule set\n",
        "    weights, biases, layers = add_hidden_units(weights, biases, layers)\n",
        "    display(layers)\n",
        "\n",
        "    # Pre-process input data\n",
        "    X = pt.tensor(preprocess_data(X, feature_names, layers)[0])\n",
        "    y = pt.tensor(y.astype(float))\n",
        "\n",
        "    print(\"Parameters 0:\")\n",
        "    display(weights)\n",
        "    display(biases)\n",
        "\n",
        "    # Construct a training model\n",
        "    model = KBANN(list(map(pt.tensor, weights)), list(map(pt.tensor, biases)))\n",
        "\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = pt.optim.Adam(model.parameters(), lr=0.1)\n",
        "\n",
        "    train_model(model, X, y, training_epochs, optimizer, criterion)\n",
        "\n",
        "    weights, biases, cluster_indices = eliminate_weights(model.weights, model.biases)\n",
        "\n",
        "    # Create second model with fixed weights - train just biases\n",
        "    model = KBANN(\n",
        "        list(map(pt.tensor, weights)), list(map(pt.tensor, biases)), fix_weights=True\n",
        "    )\n",
        "    train_model(model, X, y, training_epochs, optimizer, criterion)\n",
        "\n",
        "    # Translate network to rules\n",
        "    ruleset = network_to_rule(weights, biases, cluster_indices, layers)\n",
        "\n",
        "    print(\"Parameters 4:\")\n",
        "    display(weights)\n",
        "    display(biases)\n",
        "    print(\"Rule Extraction Finished!\")\n",
        "\n",
        "    ########################## Added Code #########################\n",
        "    print('-'*50)\n",
        "    print(\"Original Ruleset: \\n\")\n",
        "    display(ruleset)\n",
        "    print('-'*50)\n",
        "    print(\"Simplified Ruleset: \\n\")\n",
        "    simplified_rules = simplify_rules(ruleset)\n",
        "    display(simplified_rules)\n",
        "    ########################## Added Code #########################\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # CURRENT_DIRECTOR = os.getcwd()\n",
        "    CURRENT_DIRECTOR = \"/content/drive/MyDrive/ColabNotebooks/Winter Semester 2023 24/NSI/Datasets-20231011/shapes\"\n",
        "    shapes_folder = CURRENT_DIRECTOR\n",
        "    CURRENT_DIRECTOR = dir_path\n",
        "\n",
        "    # Initial parameters\n",
        "    training_epochs = 10\n",
        "\n",
        "    # atoms_to_add = [\"complete_course\", \"freshman\", \"sent_application\", \"high_gpa\"]\n",
        "    atoms_to_add = []\n",
        "    # Load training data\n",
        "    # data_file_path = os.path.join(CURRENT_DIRECTOR, \"Datasets\", \"student.txt\")\n",
        "    data_file_path = shapes_folder\n",
        "    X, y, feature_names = load_data_shapes(data_file_path)\n",
        "    main(\n",
        "        X,\n",
        "        y,\n",
        "        feature_names,\n",
        "        training_epochs,\n",
        "        os.path.join(CURRENT_DIRECTOR, \"Datasets\", \"shapes_rules.txt\"),\n",
        "        atoms_to_add,\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 546
        },
        "id": "pMC8azwzit9V",
        "outputId": "18c7b478-b9d3-4a82-fbf2-40f1e17c33b2"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['(LeftOf(a', 'c)', 'Small(a))', '(LeftOf(b', 'Small(b))', '(LeftOf(c', 'Small(c))', '(LeftOf(d', 'Small(d))', '(LeftOf(e', 'Small(e))']\n",
            "['Cube(c)5', 'Cube(c)6', 'Cube(c)7', 'Cube(c)8', 'Cube(c)9']\n",
            "['Cube(c)5', 'Cube(c)6', 'Cube(c)7', 'Cube(c)8', 'Cube(c)9']\n",
            "['Cube(c)']\n",
            "---------------------\n",
            "['(LeftOf(a', 'c)', 'Small(a))', '(LeftOf(b', 'Small(b))', '(LeftOf(c', 'Small(c))', '(LeftOf(d', 'Small(d))', '(LeftOf(e', 'Small(e))']\n",
            "['Cube(c)5', 'Cube(c)6', 'Cube(c)7', 'Cube(c)8', 'Cube(c)9', 'head1', 'head2', 'head3']\n",
            "['Cube(c)5', 'Cube(c)6', 'Cube(c)7', 'Cube(c)8', 'Cube(c)9', 'head1', 'head2', 'head3']\n",
            "['Cube(c)']\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-8de584a730ea>\u001b[0m in \u001b[0;36m<cell line: 68>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0mdata_file_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshapes_folder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data_shapes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_file_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m     main(\n\u001b[0m\u001b[1;32m     84\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-8de584a730ea>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(X, y, feature_names, training_epochs, rule_file_path, atoms_to_add)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;31m# Pre-process input data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocess_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-475cb6f37d81>\u001b[0m in \u001b[0;36mpreprocess_data\u001b[0;34m(dataset, feature_names, layers)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0;31m# input layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfeature_names\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munit\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0munit\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-475cb6f37d81>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0;31m# input layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfeature_names\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munit\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0munit\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: '(LeftOf(a' is not in list"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0280KpDAr0dd",
        "outputId": "ef1ac44c-9c02-4211-f5fa-e846083c49bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['grad_student']\n",
            "['has_superviser', 'take_course']\n",
            "['has_superviser', 'take_course']\n",
            "['scholarship_candidate']\n",
            "---------------------\n",
            "['grad_student', 'complete_course', 'freshman', 'sent_application', 'high_gpa']\n",
            "['has_superviser', 'take_course', 'head1', 'head2', 'head3']\n",
            "['has_superviser', 'take_course', 'head1', 'head2', 'head3']\n",
            "['scholarship_candidate']\n",
            "Parameters 0:\n",
            "[[4. 4. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0.]]\n",
            "[[4.]\n",
            " [4.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]]\n",
            "[[2. 2. 0. 0. 0.]]\n",
            "[[6.]]\n",
            "Epoch 0: Loss = 0.359669006\n",
            "Epoch 1: Loss = 0.219217500\n",
            "Epoch 2: Loss = 0.188250214\n",
            "Epoch 3: Loss = 0.079016455\n",
            "Epoch 4: Loss = 0.034023343\n",
            "Epoch 5: Loss = 0.025121590\n",
            "Epoch 6: Loss = 0.024761989\n",
            "Epoch 7: Loss = 0.024384248\n",
            "Epoch 8: Loss = 0.024821951\n",
            "Epoch 9: Loss = 0.024891189\n",
            "Epoch 0: Loss = 0.024918570\n",
            "Epoch 1: Loss = 0.024964388\n",
            "Epoch 2: Loss = 0.024969672\n",
            "Epoch 3: Loss = 0.024941570\n",
            "Epoch 4: Loss = 0.024825102\n",
            "Epoch 5: Loss = 0.024901476\n",
            "Epoch 6: Loss = 0.024913220\n",
            "Epoch 7: Loss = 0.024850094\n",
            "Epoch 8: Loss = 0.024911601\n",
            "Epoch 9: Loss = 0.024912555\n",
            "has_superviser = 0\n",
            "if 2.8288474748187102 < -0.6674088846821774 * nt(freshman,high_gpa) + 3.3064460795053185 * nt(grad_student) + -0.6924141849219968 * nt(complete_course) + -0.634494264645121 * nt(sent_application):\n",
            "\thas_superviser = 1\n",
            "take_course = 0\n",
            "if 2.8284344414412477 < -0.7113339040876203 * nt(sent_application) + 3.2841470653400076 * nt(grad_student) + -0.6410808427820556 * nt(high_gpa) + -0.6880641991114264 * nt(complete_course,freshman):\n",
            "\ttake_course = 1\n",
            "head1 = 0\n",
            "if -0.15599658767943553 < 0.21550334918065547 * nt(freshman,high_gpa) + -0.12521903251572156 * nt(sent_application) + 0.3428944994943588 * nt(complete_course) + 0.2863816733112299 * nt(grad_student):\n",
            "\thead1 = 1\n",
            "head2 = 0\n",
            "if -0.390255846430714 < 0.4668988235410718 * nt(grad_student,freshman) + 0.30102711618615063 * nt(sent_application) + 0.41665298917949567 * nt(high_gpa) + 0.52502641711124 * nt(complete_course):\n",
            "\thead2 = 1\n",
            "head3 = 0\n",
            "if -0.4042475952018987 < 0.45017458987110814 * nt(grad_student,high_gpa) + 0.544756791188154 * nt(freshman) + 0.3635921538100863 * nt(sent_application) + 0.5819922899634314 * nt(complete_course):\n",
            "\thead3 = 1\n",
            "scholarship_candidate = 0\n",
            "if 6.798463973627586 < -0.6914598281130573 * nt(head1) + 3.2970116354811254 * nt(take_course) + -0.761193916236301 * nt(head2,head3) + 3.227886652748445 * nt(has_superviser):\n",
            "\tscholarship_candidate = 1\n",
            "Parameters 4:\n",
            "[[ 3.30644608  3.28414707  0.28638167  0.46689882  0.45017459]\n",
            " [-0.69241418 -0.6880642   0.3428945   0.52502642  0.58199229]\n",
            " [-0.66740888 -0.6880642   0.21550335  0.46689882  0.54475679]\n",
            " [-0.63449426 -0.7113339  -0.12521903  0.30102712  0.36359215]\n",
            " [-0.66740888 -0.64108084  0.21550335  0.41665299  0.45017459]]\n",
            "[[ 3.22788665]\n",
            " [ 3.29701164]\n",
            " [-0.69145983]\n",
            " [-0.76119392]\n",
            " [-0.76119392]]\n",
            "[[ 2.82884747  2.82843444 -0.15599659 -0.39025585 -0.4042476 ]]\n",
            "[[6.79846397]]\n",
            "Rule Extraction Finished!\n",
            "--------------------------------------------------\n",
            "Original Ruleset: \n",
            "\n",
            "has_superviser :- 2.8288474748187102 < -0.6674088846821774 * nt(freshman,high_gpa) + 3.3064460795053185 * nt(grad_student) + -0.6924141849219968 * nt(complete_course) + -0.634494264645121 * nt(sent_application)\n",
            "take_course :- 2.8284344414412477 < -0.7113339040876203 * nt(sent_application) + 3.2841470653400076 * nt(grad_student) + -0.6410808427820556 * nt(high_gpa) + -0.6880641991114264 * nt(complete_course,freshman)\n",
            "head1 :- -0.15599658767943553 < 0.21550334918065547 * nt(freshman,high_gpa) + -0.12521903251572156 * nt(sent_application) + 0.3428944994943588 * nt(complete_course) + 0.2863816733112299 * nt(grad_student)\n",
            "head2 :- -0.390255846430714 < 0.4668988235410718 * nt(grad_student,freshman) + 0.30102711618615063 * nt(sent_application) + 0.41665298917949567 * nt(high_gpa) + 0.52502641711124 * nt(complete_course)\n",
            "head3 :- -0.4042475952018987 < 0.45017458987110814 * nt(grad_student,high_gpa) + 0.544756791188154 * nt(freshman) + 0.3635921538100863 * nt(sent_application) + 0.5819922899634314 * nt(complete_course)\n",
            "scholarship_candidate :- 6.798463973627586 < -0.6914598281130573 * nt(head1) + 3.2970116354811254 * nt(take_course) + -0.761193916236301 * nt(head2,head3) + 3.227886652748445 * nt(has_superviser)\n",
            "--------------------------------------------------\n",
            "Simplified Ruleset: \n",
            "\n",
            "has_superviser Rule:\n",
            "A person has_superviser if the weighted sum of the following conditions is -0.6674088846821774 * nt(freshman,high_gpa) than 2.829.\n",
            "Conditions:\n",
            "Not nt(grad_student)) weighted by 3.306\n",
            "Not nt(complete_course)) weighted by -0.692\n",
            "Not nt(sent_application)) weighted by -0.634\n",
            "\n",
            "take_course Rule:\n",
            "A person take_course if the weighted sum of the following conditions is -0.7113339040876203 * nt(sent_application) than 2.828.\n",
            "Conditions:\n",
            "Not nt(grad_student)) weighted by 3.284\n",
            "Not nt(high_gpa)) weighted by -0.641\n",
            "Not nt(complete_course(freshman)) weighted by -0.688\n",
            "\n",
            "head1 Rule:\n",
            "A person head1 if the weighted sum of the following conditions is 0.21550334918065547 * nt(freshman,high_gpa) than -0.156.\n",
            "Conditions:\n",
            "Not nt(sent_application)) weighted by -0.125\n",
            "Not nt(complete_course)) weighted by 0.343\n",
            "Not nt(grad_student)) weighted by 0.286\n",
            "\n",
            "head2 Rule:\n",
            "A person head2 if the weighted sum of the following conditions is 0.4668988235410718 * nt(grad_student,freshman) than -0.390.\n",
            "Conditions:\n",
            "Not nt(sent_application)) weighted by 0.301\n",
            "Not nt(high_gpa)) weighted by 0.417\n",
            "Not nt(complete_course)) weighted by 0.525\n",
            "\n",
            "head3 Rule:\n",
            "A person head3 if the weighted sum of the following conditions is 0.45017458987110814 * nt(grad_student,high_gpa) than -0.404.\n",
            "Conditions:\n",
            "Not nt(freshman)) weighted by 0.545\n",
            "Not nt(sent_application)) weighted by 0.364\n",
            "Not nt(complete_course)) weighted by 0.582\n",
            "\n",
            "scholarship_candidate Rule:\n",
            "A person scholarship_candidate if the weighted sum of the following conditions is -0.6914598281130573 * nt(head1) than 6.798.\n",
            "Conditions:\n",
            "Not nt(take_course)) weighted by 3.297\n",
            "Not nt(head2(head3)) weighted by -0.761\n",
            "Not nt(has_superviser)) weighted by 3.228\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-15-a64470a98a30>:17: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  layers = np.array(layers)\n"
          ]
        }
      ],
      "source": [
        "def main(\n",
        "    X,\n",
        "    y,\n",
        "    feature_names,\n",
        "    training_epochs,\n",
        "    rule_file_path,\n",
        "    atoms_to_add,\n",
        "):\n",
        "    # Translate rules to a network\n",
        "\n",
        "    ruleset = load_rules(rule_file_path)\n",
        "    ruleset = rewrite_rules(ruleset)\n",
        "    weights, biases, layers = rule_to_network(ruleset)\n",
        "\n",
        "    display(layers)\n",
        "    print(\"---------------------\")\n",
        "    # Add input features not referred by the rule set\n",
        "    # weights, layers = add_input_units(weights, layers, ['complete_course'])\n",
        "    weights, layers = add_input_units(weights, layers, atoms_to_add)\n",
        "\n",
        "    # Add hidden units not specified by the initial rule set\n",
        "    weights, biases, layers = add_hidden_units(weights, biases, layers)\n",
        "    display(layers)\n",
        "\n",
        "    # Pre-process input data\n",
        "    X = pt.tensor(preprocess_data(X, feature_names, layers)[0])\n",
        "    y = pt.tensor(y.astype(float))\n",
        "\n",
        "    print(\"Parameters 0:\")\n",
        "    display(weights)\n",
        "    display(biases)\n",
        "\n",
        "    # Construct a training model\n",
        "    model = KBANN(list(map(pt.tensor, weights)), list(map(pt.tensor, biases)))\n",
        "\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = pt.optim.Adam(model.parameters(), lr=0.1)\n",
        "\n",
        "    train_model(model, X, y, training_epochs, optimizer, criterion)\n",
        "\n",
        "    weights, biases, cluster_indices = eliminate_weights(model.weights, model.biases)\n",
        "\n",
        "    # Create second model with fixed weights - train just biases\n",
        "    model = KBANN(\n",
        "        list(map(pt.tensor, weights)), list(map(pt.tensor, biases)), fix_weights=True\n",
        "    )\n",
        "    train_model(model, X, y, training_epochs, optimizer, criterion)\n",
        "\n",
        "    # Translate network to rules\n",
        "    ruleset = network_to_rule(weights, biases, cluster_indices, layers)\n",
        "\n",
        "    print(\"Parameters 4:\")\n",
        "    display(weights)\n",
        "    display(biases)\n",
        "    print(\"Rule Extraction Finished!\")\n",
        "\n",
        "    ########################## Added Code #########################\n",
        "    print('-'*50)\n",
        "    print(\"Original Ruleset: \\n\")\n",
        "    display(ruleset)\n",
        "    print('-'*50)\n",
        "    print(\"Simplified Ruleset: \\n\")\n",
        "    simplified_rules = simplify_rules(ruleset)\n",
        "    display(simplified_rules)\n",
        "    ########################## Added Code #########################\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # CURRENT_DIRECTOR = os.getcwd()\n",
        "    CURRENT_DIRECTOR = dir_path\n",
        "\n",
        "    # Initial parameters\n",
        "    training_epochs = 10\n",
        "\n",
        "    atoms_to_add = [\"complete_course\", \"freshman\", \"sent_application\", \"high_gpa\"]\n",
        "    # Load training data\n",
        "    data_file_path = os.path.join(CURRENT_DIRECTOR, \"Datasets\", \"student.txt\")\n",
        "    X, y, feature_names = load_data(data_file_path)\n",
        "    main(\n",
        "        X,\n",
        "        y,\n",
        "        feature_names,\n",
        "        training_epochs,\n",
        "        os.path.join(CURRENT_DIRECTOR, \"Datasets\", \"student_rules.txt\"),\n",
        "        atoms_to_add,\n",
        "    )"
      ]
    }
  ]
}