{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "NbNBLFYNlesf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "fLs1nvafUinz"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tqSM1wCrUh8y",
        "outputId": "2d201068-2a41-4189-bfaa-21d820d08149"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset"
      ],
      "metadata": {
        "id": "kBPI5lDclg-Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "email_data_path = '/content/drive/MyDrive/ColabNotebooks/Winter Semester 2023 24/NSI/Datasets-20231011/email/'\n",
        "n = 50"
      ],
      "metadata": {
        "id": "-QS7RfQeUp2E"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_email_data(folder_path, no_of_emails=n):\n",
        "    emails = []\n",
        "    labels = []  # Spam 1, Ham 0\n",
        "    counter = 1\n",
        "\n",
        "    # Iterate through ham emails\n",
        "    ham_folder_path = os.path.join(folder_path, 'ham')\n",
        "    for filename in os.listdir(ham_folder_path):\n",
        "        if counter >= no_of_emails:\n",
        "            break\n",
        "\n",
        "        with open(os.path.join(ham_folder_path, filename), 'r', encoding='utf-8', errors='ignore') as file:\n",
        "            lines = file.readlines()\n",
        "            if len(lines) >= 3:\n",
        "                body = ''.join(lines[2:]).strip()\n",
        "                emails.append(body)\n",
        "                labels.append(0)\n",
        "                counter += 1\n",
        "\n",
        "    counter = 1\n",
        "\n",
        "    # Iterate through spam emails\n",
        "    spam_folder_path = os.path.join(folder_path, 'spam')\n",
        "    for filename in os.listdir(spam_folder_path):\n",
        "        if counter >= no_of_emails:\n",
        "            break\n",
        "\n",
        "        with open(os.path.join(spam_folder_path, filename), 'r', encoding='utf-8', errors='ignore') as file:\n",
        "            lines = file.readlines()\n",
        "            if len(lines) >= 3:\n",
        "                body = ''.join(lines[2:]).strip()\n",
        "                emails.append(body)\n",
        "                labels.append(1)\n",
        "                counter += 1\n",
        "\n",
        "    return emails, labels"
      ],
      "metadata": {
        "id": "C8jqXW8YU5Oh"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_emails, train_labels =  parse_email_data(os.path.join(email_data_path, 'train'), n)\n",
        "# test_emails, test_labels =  parse_email_data(os.path.join(email_data_path, 'test'), n)\n",
        "# val_emails, val_labels =  parse_email_data(os.path.join(email_data_path, 'val'), n)"
      ],
      "metadata": {
        "id": "7MIafjuZq3Pn"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "OjPXy_2bqowl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_context_vector(context, word_to_ix):\n",
        "    idxs = [word_to_ix[w] for w in context]\n",
        "    return torch.tensor(idxs, dtype=torch.long)"
      ],
      "metadata": {
        "id": "4P7uvQYmqrH7"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EMDEDDING_DIM = 128\n",
        "\n",
        "vocab = set(word for email in train_emails for word in email)\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "word_to_ix = {word: ix for ix, word in enumerate(vocab)}\n",
        "ix_to_word = {ix: word for ix, word in enumerate(vocab)}"
      ],
      "metadata": {
        "id": "7_lv3shOxL3E"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Commented out parts = actual CBOW model (taken from the Github link provided)\n",
        "class CBOW(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super(CBOW, self).__init__()\n",
        "\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.linear1 = nn.Linear(embedding_dim, 256)\n",
        "        self.activation_function1 = nn.ReLU()\n",
        "        # self.linear2 = nn.Linear(128, vocab_size)\n",
        "        # self.activation_function2 = nn.LogSoftmax(dim = -1)\n",
        "\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embeds = sum(self.embeddings(inputs)).view(1,-1)\n",
        "        out = self.linear1(embeds)\n",
        "        out = self.activation_function1(out)\n",
        "        # out = self.linear2(out)\n",
        "        # out = self.activation_function2(out)\n",
        "        return out\n",
        "\n",
        "    def get_word_embedding(self, word):\n",
        "        word = torch.tensor([word_to_ix[word]])\n",
        "        return self.embeddings(word).view(1, -1)"
      ],
      "metadata": {
        "id": "6H8pizcaxRCh"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SpamClassifier(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(SpamClassifier, self).__init__()\n",
        "        self.linear = nn.Linear(input_size, 1)\n",
        "        self.activation_function = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        out = self.linear(inputs)\n",
        "        out = self.activation_function(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "Q0J-dCu2xT7N"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the models\n",
        "cbow_model = CBOW(vocab_size, EMDEDDING_DIM)\n",
        "\n",
        "classifier_model = SpamClassifier(64)\n",
        "\n",
        "loss_function = nn.BCELoss()\n",
        "optimizer = torch.optim.SGD(classifier_model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "NNyzcZFoxbzA"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## For original CBOW"
      ],
      "metadata": {
        "id": "NsRpSw-qW1Nt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = CBOW(vocab_size, EMDEDDING_DIM)\n",
        "\n",
        "loss_function = nn.NLLLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
        "\n",
        "#TRAINING\n",
        "for epoch in range(50):\n",
        "    total_loss = 0\n",
        "\n",
        "    for context, target in data:\n",
        "        context_vector = make_context_vector(context, word_to_ix)\n",
        "\n",
        "        log_probs = model(context_vector)\n",
        "\n",
        "        total_loss += loss_function(log_probs, torch.tensor([word_to_ix[target]]))\n",
        "\n",
        "    #optimize at the end of each epoch\n",
        "    optimizer.zero_grad()\n",
        "    total_loss.backward()\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "id": "TQ7kWcpuWqnC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "4EA7WXd-Ur_0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TRAINING\n",
        "for epoch in range(2):\n",
        "    total_loss = 0\n",
        "    correct_predictions = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    for i, (email, label) in enumerate(zip(train_emails, train_labels)):\n",
        "        for j in range(2, len(email) - 2):\n",
        "            context = [email[j - 2], email[j - 1],\n",
        "                       email[j + 1], email[j + 2]]\n",
        "            target = email[j]\n",
        "\n",
        "            context_vector = make_context_vector(context, word_to_ix)\n",
        "\n",
        "            # Use CBOW representation for classification\n",
        "            cbow_representation = cbow_model(context_vector)\n",
        "            output = classifier_model(cbow_representation)\n",
        "\n",
        "            # labels (0 for 'ham', 1 for 'spam')\n",
        "            label_tensor = torch.tensor([[label]], dtype=torch.float32)\n",
        "\n",
        "            # Compute loss and backpropagate\n",
        "            loss = loss_function(output, label_tensor)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Calculate accuracy\n",
        "            predicted_label = 1 if output.item() > 0.5 else 0\n",
        "\n",
        "            if (predicted_label == label):\n",
        "              correct_predictions += 1\n",
        "\n",
        "            total_samples += 1\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = correct_predictions / total_samples\n",
        "\n",
        "    print(f'Epoch {epoch + 1}, Accuracy: {accuracy * 100:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E52WQvGmSsrx",
        "outputId": "01258ac6-9071-4fc5-9ea5-ff287ee66e5b"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Accuracy: 99.67%\n",
            "Epoch 2, Accuracy: 99.37%\n"
          ]
        }
      ]
    }
  ]
}